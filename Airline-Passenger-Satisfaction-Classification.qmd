---
title: "Airline Passenger Satisfaction Classification"
author: "Joaquin Sanchez Ibarra"
format: html
editor: visual
---

Load packages.

```{r}
pacman::p_load(tidyverse, data.table, DataExplorer, missForest, caret, doParallel, foreach, e1071, car, randomForest, gbm, gt, bartMachine,skimr)
```


```{r}
df <- fread("./data/train.csv")
```

```{r}
#glimpse(df)
df %>% skim()
```
Clean data

Replace white space with underscore
```{r}
colnames(df) <- gsub(c("\\s+"), "_", colnames(df)) 
colnames(df) <-   gsub("/", "_", colnames(df))
  
```


```{r}
df1 <- df %>% 
  select(-V1, -id) %>% 
  slice_sample(prop = .1,replace = F) %>% 
  mutate(Gender = as.factor(Gender),
         Customer_Type =  as.factor(Customer_Type),
         Type_of_Travel = as.factor(Type_of_Travel),
         Class = factor(Class),
         satisfaction = factor(satisfaction))
summary(df1)
```


Missing NA plot
```{r}
DataExplorer::plot_missing(df1)
```

Imputation of NA values using `missForest` package

```{r, eval=F}
rf_na <- missForest(df1,
                    ntree = 100,
                    variablewise = F,
                    verbose= T,
                    mtry = round(sqrt(ncol(df1)-1)))



```

```{r}
df2 <- rf_na$ximp
```

```{r}
df2 <- slice_sample(df2,n = 1000)
x <- model.matrix(satisfaction ~ ., data = df2)[,-1] 
y <- df2$satisfaction
```

```{r}
df2 <- data.frame(x,y) 
colnames(df2) <-  gsub("\\.","", colnames(data.frame(cbind(x,y)) ))
df2 <- mutate(rename(df2, "satisfaction" = "y"))

plot_missing(df2)
```


```{r}
df2$satisfaction <- factor(df2$satisfaction, levels = c("satisfied", "neutral or dissatisfied"), labels = c("satisfied", "Neutral_or_dissatisfied"))
levels(df2$satisfaction)
```


KNN model
```{r}
set.seed(1)

index <- caret::createDataPartition(df2$satisfaction, p = .7,list = FALSE)
train <- df2[index,]
test <- df2[-index,]

# Set up the training control
ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,  # Compute class probabilities for AUC
                     savePredictions = TRUE)

# Define the tuning grid
grid <- expand.grid(k = 1:100)


knn_model <- train(satisfaction ~ ., data = train,
      method = "knn",
      trControl = ctrl,
      preProcess = c("center", "scale"),
      tuneGrid = grid,
      tuneLength = 5)


```


Results
```{r}
knn_model
plot(knn_model)

knn_model$results

# test predictions
test_pred <- predict(knn_model, newdata = test)

confusionMatrix(test$satisfaction ,test_pred)
```


Bagging and RF models
```{r}
set.seed(1)
param_grid <-  expand.grid(mtry = c(2, 3, 4, (ncol(df2)-1) ), ntree = c(100, 200, 300,500))
ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,  # Compute class probabilities for AUC
                     savePredictions = TRUE)

rf_model <- train(satisfaction ~ ., data = train,
                  method = "rf",
                  trControl = ctrl,
                  tune_Grid = param_grid,
                  importance = TRUE)


plot(rf_model)

rf_model$results

# test predictions
rf_test_pred <- predict(rf_model, newdata = test)

confusionMatrix(test$satisfaction ,rf_test_pred)

varImp(rf_model)
varImpPlot(rf_model$finalModel)
importance(rf_model$finalModel) %>% data.frame() %>% gt() 
```

Boosting model 
```{r}
set.seed(1)
# Specify grid of hyperparameters to search over
grid <- expand.grid(interaction.depth = c(1, 2, 3),
                    n.trees = c(50, 100, 200),
                    shrinkage = c(0.01, 0.1, 0.5),
                    n.minobsinnode = c(5, 10, 20))

ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,  # Compute class probabilities for AUC
                     savePredictions = TRUE)


gbm_fit <- train(satisfaction ~ ., data = train, method = "gbm", trControl = ctrl, tuneGrid = grid, verbose = T)

gbm_fit
summary(gbm_fit)

gbm_fit$finalModel


# test predictions
gmb_test_pred <- predict(gbm_fit, newdata = test)
confusionMatrix(test$satisfaction ,gmb_test_pred)
```


BART Bayesian additive regression tress

```{r}
set.seed(1)

# Specify grid of hyperparameters to search over
grid <- expand.grid(
  k = c(100, 200),
  burn = c(100),
  m = c(1000)
)

grid <- expand.grid(num_trees = c(200),
                         k = c(2, 3, 4),
                         alpha = c(0.95, 0.99),
                         beta = c(2, 4),
                         nu = c(0.1, 0.2))


ctrl <- trainControl(method = "cv",
                     number = 10,
                     classProbs = TRUE,  # Compute class probabilities for AUC
                     savePredictions = TRUE,
                     search = "random",        # Random search method for hyperparameters
                     allowParallel = TRUE,)

bartFit <- train(
  satisfaction ~ ., 
  data = train, 
  method = "bartMachine",
  trControl = ctrl,
  tuneGrid = grid,
  burn = 1000,
  iter = 4000
)
```


```{r}
plot(bartFit)

bartFit$results

# test predictions
bart_test_pred <- predict(bartFit, newdata = test)

confusionMatrix(test$satisfaction ,bart_test_pred)


bart_importance <- varImp(bartFit) 

bart_importance_df <- bart_importance$importance %>% 
  data.frame() %>%
  rownames_to_column(., var = "Variables") %>%
  head(5) %>% 
  gt()  

bart_importance_df 
```


Check for multicollinearity

```{r}
pacman::p_load(corrplot,Hmisc)

col_id <- which(names(df2) == c("satisfaction|GenderMale"))
# Calculate the correlation matrix
numeric_df <- df2[, sapply(df2, is.numeric)]

cor_matrix <- cor(numeric_df)

# Create the correlation plot
corrplot(cor_matrix, type = "upper", order = "hclust", tl.cex = 0.8)

# Create the correlation table
(corr_table <- rcorr(as.matrix(numeric_df)))

corr_table$r[corr_table$r>.5]

```




Set up to use DoParallel.

```{r}
cl <- makeCluster(detectCores())
registerDoParallel(cl)
```

```{r}
set.seed(1)

index <- caret::createDataPartition(df2$satisfaction, p = .7,list = FALSE)
train <- df2[index,]
test <- df2[-index,]



# Define the hyperparameters to be tuned
tune_grid <- expand.grid(kernel = c("linear", "polynomial", "radial"),
                         cost = c(0.1, 1, 10, 100),
                         gamma = c(0.01, 0.1, 1))

# Use the tune function to train the model and tune the hyperparameters
tune.out <- tune(svm, satisfaction ~ ., data = train,
                  ranges = tune_grid,
                  tunecontrol = tune.control(sampling = "cross", cross = 5))


summary(tune.out)

```

```{r}
svm_pre <- predict(tune.out$best.model, newdata = test)

attributes(svm_pre)$decision.values

confusionMatrix(reference = test$satisfaction, svm_pre)

```


```{r}
# Stop the parallel 
stopCluster(cl)
```

Run after using DoParallel

```{r}
unregister_dopar <- function() {
  env <- foreach:::.foreachGlobals
  rm(list=ls(name=env), pos=env)
}
unregister_dopar()
```
